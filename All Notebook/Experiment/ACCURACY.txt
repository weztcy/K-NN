ACCURACY:

Berdasarkan hasil pengujian yang ditampilkan dalam tabel, terlihat bahwa proses normalisasi memiliki pengaruh signifikan terhadap performa algoritma K-Nearest Neighbors (KNN). Tanpa penerapan normalisasi, akurasi tertinggi hanya mencapai 0.7597 pada saat jumlah tetangga (K) bernilai 11. Di sisi lain, akurasi mengalami fluktuasi dengan pola yang tidak konsisten seiring bertambahnya nilai K. Sebagai contoh, nilai akurasi sempat turun pada K = 7 menjadi 0.7273, setara dengan nilai awal pada K = 1. Hal ini menunjukkan bahwa tanpa normalisasi, perhitungan jarak antar data bisa terganggu oleh fitur yang memiliki skala besar, sehingga berdampak langsung pada kualitas prediksi KNN.
Ketika metode Min-Max Scaling diterapkan, terdapat peningkatan akurasi yang signifikan hampir di seluruh nilai K. Puncaknya terjadi pada K = 7 dengan akurasi sebesar 0.8117, yang merupakan akurasi tertinggi di seluruh eksperimen. Ini menunjukkan bahwa Min-Max Scaling berhasil membawa seluruh fitur ke dalam rentang nilai yang seragam (biasanya 0 hingga 1), sehingga tidak ada fitur yang mendominasi perhitungan jarak. Konsistensi peningkatan akurasi dari K = 3 hingga K = 9 juga menunjukkan bahwa Min-Max Scaling memberi kestabilan performa pada model KNN. Metode ini sangat cocok digunakan saat fitur memiliki satuan atau rentang nilai yang berbeda-beda.
Sementara itu, penerapan Z-Score (Standard Scaling) juga memberikan hasil yang cukup kompetitif. Dengan akurasi tertinggi sebesar 0.7922 pada K = 7, metode ini menunjukkan bahwa normalisasi berbasis distribusi (dengan mengurangi nilai rata-rata dan membagi dengan standar deviasi) juga efektif untuk mengurangi pengaruh fitur yang tidak seimbang. Tren akurasi dengan metode ini cenderung stabil, dengan fluktuasi yang kecil. Misalnya, pada K = 3 dan K = 5 akurasinya tidak berbeda jauh, masing-masing 0.7403 dan 0.7468. Keunggulan dari Z-Score adalah kemampuannya mempertahankan struktur distribusi data meskipun sudah dinormalisasi, cocok untuk data dengan outlier ringan.
Sebaliknya, metode Decimal Scaling menunjukkan performa yang kurang memuaskan. Akurasi tertinggi yang diperoleh hanya 0.7403 pada K = 9, yang bahkan lebih rendah dari hasil terbaik tanpa normalisasi. Akurasi cenderung tidak stabil dan cenderung lebih rendah pada semua nilai K. Ini bisa disebabkan oleh sifat Decimal Scaling yang hanya menggeser titik desimal tanpa benar-benar menyesuaikan distribusi atau rentang nilai fitur. Akibatnya, metode ini kurang mampu menangani data dengan variasi nilai yang tinggi atau distribusi yang tidak seragam, sehingga tidak cocok digunakan dalam konteks model berbasis jarak seperti KNN.
Secara keseluruhan, hasil ini menegaskan bahwa normalisasi data memiliki peran penting dalam meningkatkan performa KNN. Di antara keempat metode yang diuji, Min-Max Scaling memberikan hasil terbaik, diikuti oleh Z-Score, sementara Tanpa Normalisasi dan Decimal Scaling menunjukkan performa yang relatif lebih rendah. Oleh karena itu, dalam praktik pengembangan model KNN, sangat disarankan untuk menerapkan teknik normalisasi yang sesuai dengan karakteristik data yang digunakan, terutama ketika fitur memiliki skala yang berbeda.




PRECISION:

Dari hasil evaluasi berdasarkan metrik Precision, terlihat bahwa metode normalisasi memberikan pengaruh nyata terhadap performa algoritma K-Nearest Neighbors (KNN). Sama seperti pada metrik akurasi sebelumnya, precision juga meningkat ketika dilakukan normalisasi data, terutama menggunakan Min-Max Scaling dan Z-Score.
Pada data tanpa normalisasi, precision tertinggi yang diperoleh adalah 0.7390 pada K = 11, sedangkan nilai precision terendah terjadi pada K = 7 sebesar 0.7065. Meskipun precision mengalami sedikit peningkatan pada nilai K yang lebih besar, tren kenaikannya tidak terlalu signifikan. Ini mengindikasikan bahwa tanpa normalisasi, model KNN kurang optimal dalam mengklasifikasikan data secara tepat, terutama pada data yang memiliki fitur dengan skala berbeda.
Ketika diterapkan Min-Max Scaling, terjadi peningkatan precision yang cukup drastis. Precision tertinggi mencapai 0.7946 pada K = 7, dan nilai precision secara konsisten lebih tinggi daripada metode tanpa normalisasi. Ini sejalan dengan pengamatan pada metrik akurasi, di mana Min-Max Scaling membantu memperjelas perbedaan antar data dengan menyamakan skala fitur, sehingga KNN dapat menentukan tetangga terdekat dengan lebih akurat. Precision tetap tinggi di berbagai nilai K, yang menunjukkan kestabilan dan keandalan metode ini.
Untuk metode Z-Score (Standard Scaling), hasil precision tertinggi adalah 0.7737 pada K = 7, sedikit lebih rendah dari Min-Max Scaling namun tetap jauh lebih baik dibandingkan tanpa normalisasi. Pola precision dari K = 3 hingga K = 11 cenderung naik secara konsisten. Hal ini menunjukkan bahwa Z-Score juga merupakan metode yang efektif untuk skenario KNN, terutama ketika data memiliki distribusi yang tidak merata. Z-Score mempertahankan karakteristik distribusi data sambil menyamakan pengaruh setiap fitur.
Sebaliknya, Decimal Scaling kembali menunjukkan performa paling rendah. Precision tertinggi hanya 0.7172, yang bahkan lebih rendah dibanding precision terbaik tanpa normalisasi. Seluruh nilai precision pada metode ini cenderung lebih rendah dan tidak stabil. Ini menunjukkan bahwa menggeser titik desimal saja tidak cukup untuk membuat skala fitur menjadi seimbang atau relevan bagi KNN. Metode ini mungkin terlalu sederhana dan tidak mempertimbangkan distribusi nilai data secara menyeluruh.
Secara keseluruhan, hasil analisis ini kembali menegaskan bahwa normalisasi sangat penting untuk meningkatkan performa KNN, tidak hanya pada akurasi tetapi juga pada precision. Min-Max Scaling tetap menjadi metode terbaik yang mampu memberikan hasil paling stabil dan tinggi. Z-Score juga merupakan alternatif yang kuat, terutama ketika mempertahankan distribusi data menjadi penting. Oleh karena itu, pemilihan metode normalisasi yang tepat dapat secara signifikan memengaruhi kualitas klasifikasi dalam model KNN, terutama dalam konteks data multivariat yang kompleks.




RECALL:
Berdasarkan hasil evaluasi menggunakan metrik Recall, kita kembali melihat pola yang konsisten terkait pengaruh metode normalisasi terhadap performa algoritma K-Nearest Neighbors (KNN). Recall merupakan metrik penting untuk menilai sejauh mana model mampu menangkap seluruh data positif yang relevan—dan dalam konteks klasifikasi, ini sangat penting ketika konsekuensi dari false negative besar (misalnya dalam diagnosis penyakit atau deteksi penipuan).
Pada data tanpa normalisasi, nilai recall tertinggi tercapai pada K = 11 dengan 0.7444, sedangkan nilai terendah terjadi pada K = 1 sebesar 0.7030. Meskipun ada tren kenaikan secara bertahap, performanya masih tergolong sedang. Hal ini kembali mengindikasikan bahwa tanpa menyamakan skala antar fitur, model KNN kesulitan mengenali pola secara optimal, terutama jika fitur-fitur memiliki skala yang berbeda secara signifikan.
Dengan penerapan Min-Max Scaling, recall mengalami peningkatan signifikan di semua nilai K. Nilai tertinggi tercatat pada K = 7 dengan 0.7970, menjadikannya sebagai nilai recall tertinggi dari seluruh eksperimen. Nilai ini mencerminkan bahwa Min-Max Scaling secara efektif meningkatkan kemampuan model dalam mengenali data positif secara menyeluruh. Selain itu, nilai recall juga sangat stabil di atas 0.75 sejak K = 3 ke atas, yang menunjukkan kestabilan performa. Ini menguatkan bukti bahwa metode ini sangat cocok untuk digunakan dalam skenario klasifikasi berbasis jarak.
Z-Score (Standard Scaling) juga menunjukkan performa yang solid. Recall tertinggi tercapai pada K = 11 dengan 0.7586, hanya terpaut sedikit dari Min-Max Scaling. Performa recall terus meningkat seiring dengan bertambahnya nilai K, menunjukkan bahwa normalisasi berbasis distribusi (mean = 0, std = 1) juga efektif dalam meningkatkan sensitivitas model terhadap kelas positif. Metode ini menjadi alternatif kuat jika data mengandung nilai ekstrem (outlier) karena Z-Score relatif lebih tangguh dalam menjaga keseimbangan distribusi.
Sebaliknya, Decimal Scaling kembali menjadi metode dengan performa terendah. Nilai recall tertinggi hanya 0.7172 pada K = 9, dan selebihnya berada di bawah angka tersebut. Bahkan pada K = 3 dan K = 11, nilai recall turun ke 0.6909 dan 0.6990—lebih rendah daripada performa tanpa normalisasi. Hal ini menunjukkan bahwa metode ini kurang mampu menyesuaikan skala fitur dengan baik sehingga membatasi kemampuan KNN dalam mengenali seluruh data positif.
Secara keseluruhan, analisis ini menegaskan bahwa normalisasi meningkatkan performa recall KNN secara signifikan, terutama dengan Min-Max Scaling sebagai metode terbaik di antara yang diuji. Z-Score juga menunjukkan performa yang kuat dan konsisten, menjadikannya alternatif yang layak. Sedangkan Decimal Scaling terbukti kurang efektif dan bahkan tidak memberikan perbaikan berarti dibandingkan tanpa normalisasi. Maka, untuk aplikasi klasifikasi yang menuntut sensitivitas tinggi terhadap kelas positif, pemilihan metode normalisasi yang tepat menjadi faktor kunci keberhasilan model.




F1-SCORE
Dari hasil pengujian metrik F1-Score, kita kembali melihat konsistensi pengaruh metode normalisasi terhadap performa algoritma K-Nearest Neighbors (KNN). F1-Score merupakan metrik yang menyeimbangkan precision dan recall, sehingga sangat berguna untuk menilai kemampuan model dalam menangani data tidak seimbang atau saat kesalahan klasifikasi memiliki dampak besar.
Pada kondisi tanpa normalisasi, F1-Score tertinggi tercapai pada K = 11 dengan nilai 0.7414. Namun, performa F1-Score cenderung stagnan dan tidak mengalami peningkatan signifikan, dengan nilai berkisar antara 0.7030 hingga 0.7334. Ini mengindikasikan bahwa tanpa penyelarasan skala fitur, performa KNN dalam mengidentifikasi dan mengklasifikasikan data tidak optimal. Meskipun ada sedikit peningkatan pada nilai K yang lebih tinggi, hal ini tidak cukup untuk menyamai performa model dengan data yang dinormalisasi.
Metode Min-Max Scaling kembali menunjukkan performa terbaik dengan F1-Score tertinggi sebesar 0.7958 pada K = 7. Ini sekaligus menjadi nilai F1-Score tertinggi dari seluruh kombinasi yang diuji. Nilai F1-Score lainnya juga sangat stabil dan tinggi, mulai dari 0.7615 (K = 3) hingga 0.7799 (K = 9). Kestabilan ini menunjukkan bahwa Min-Max Scaling tidak hanya meningkatkan precision atau recall secara terpisah, tetapi juga menghasilkan keseimbangan yang baik antara keduanya. Hal ini memperkuat kesimpulan bahwa skala fitur yang seragam sangat membantu KNN dalam menghitung jarak secara akurat.
Untuk metode Z-Score (Standard Scaling), nilai F1-Score juga meningkat secara signifikan dibanding tanpa normalisasi. Nilai tertinggi tercatat pada K = 11 dengan 0.7554, sedikit lebih rendah dari Min-Max, tetapi cukup stabil pada semua nilai K. Metode ini sangat efektif terutama pada data yang memiliki distribusi normal atau mendekati normal. Z-Score juga bekerja baik untuk mengurangi efek outlier ringan, sehingga bisa menjadi alternatif yang kuat jika Min-Max Scaling kurang cocok.
Sebaliknya, Decimal Scaling menunjukkan performa yang paling rendah. F1-Score tertinggi hanya 0.7172 pada K = 9, dan sebagian besar nilai lainnya berada di bawah angka tersebut. Bahkan pada K = 3 dan 5, nilai F1-Score hanya mencapai 0.6840 dan 0.6995. Rendahnya performa ini mengindikasikan bahwa metode ini tidak berhasil menyesuaikan skala fitur secara efektif. Teknik penggeseran titik desimal tidak cukup untuk menjadikan fitur-fitur tersebut sebanding dalam konteks perhitungan jarak, terutama dalam model seperti KNN.
Secara keseluruhan, evaluasi menggunakan F1-Score kembali menguatkan temuan dari metrik sebelumnya bahwa normalisasi merupakan langkah krusial dalam meningkatkan performa KNN. Min-Max Scaling masih menjadi pilihan utama karena mampu memberikan keseimbangan terbaik antara precision dan recall. Z-Score menawarkan alternatif yang solid, khususnya dalam situasi data yang memiliki variasi nilai ekstrem. Sementara itu, Decimal Scaling sebaiknya dihindari atau hanya digunakan pada kasus yang sangat spesifik. Dalam penerapan KNN, pemilihan metode normalisasi yang tepat akan menentukan efektivitas dan akurasi model secara keseluruhan.




SPECIFICITY
Berdasarkan hasil pengujian dengan metrik Specificity, kita bisa melihat kembali dampak yang konsisten dari penerapan metode normalisasi terhadap performa model K-Nearest Neighbors (KNN). Specificity mengukur kemampuan model dalam mengenali data negatif secara benar, yang sangat penting dalam konteks klasifikasi biner ketika false positive harus ditekan seminimal mungkin—misalnya dalam deteksi penyakit langka atau sistem keamanan.
Pada data tanpa normalisasi, nilai specificity tertinggi adalah 0.7980, yang tercapai pada K = 9 dan K = 11. Di sisi lain, nilai specificity sempat turun ke 0.7576 pada K = 7. Meskipun terlihat cukup stabil, nilainya cenderung lebih rendah dibandingkan metode yang menggunakan normalisasi. Hal ini menunjukkan bahwa ketika fitur memiliki skala berbeda-beda, model KNN dapat mengalami kesulitan dalam membedakan data negatif dengan benar, sehingga terjadi peningkatan false positive.
Ketika menggunakan Min-Max Scaling, peningkatan specificity sangat terlihat. Nilai specificity tertinggi mencapai 0.8485 pada K = 7 dan K = 9, jauh lebih tinggi dibandingkan tanpa normalisasi. Bahkan pada K = 3 dan K = 5, nilai specificity mencapai 0.8182 dan 0.8283, menunjukkan bahwa Min-Max Scaling memberikan kontribusi besar dalam membantu model mengenali data negatif secara akurat. Hal ini sangat mungkin terjadi karena Min-Max Scaling menyamakan skala seluruh fitur ke dalam rentang yang konsisten (biasanya 0–1), sehingga pengukuran jarak antar data menjadi lebih presisi dan tidak bias terhadap fitur dengan skala besar.
Z-Score (Standard Scaling) juga menunjukkan performa yang solid. Nilai tertingginya tercapai pada K = 7 dan K = 9 dengan nilai 0.8384, yang sedikit di bawah Min-Max Scaling. Nilai specificity juga cukup stabil di atas 0.80 pada K = 7, 9, dan 11. Hal ini memperlihatkan bahwa Z-Score mampu mempertahankan struktur distribusi data sekaligus mereduksi efek dominasi dari fitur tertentu, membuat model lebih seimbang dalam mengenali data negatif maupun positif.
Sebaliknya, Decimal Scaling kembali menunjukkan performa paling rendah. Nilai specificity tertinggi hanya 0.7980 pada K = 9 dan K = 11, yang bahkan setara dengan hasil terbaik tanpa normalisasi. Nilai specificity pada K = 3 hanya 0.7273, terendah dari seluruh eksperimen. Ini memperkuat argumen bahwa metode ini kurang efektif dalam menyamakan skala fitur dalam konteks model berbasis jarak. Proses menggeser titik desimal tanpa mempertimbangkan penyebaran atau distribusi nilai membuat fitur tetap tidak seimbang, sehingga memengaruhi akurasi identifikasi data negatif.
Secara keseluruhan, evaluasi specificity memperkuat temuan sebelumnya bahwa normalisasi sangat penting untuk meningkatkan performa model KNN, tidak hanya dalam hal akurasi dan recall, tapi juga dalam menghindari false positive melalui specificity. Min-Max Scaling kembali menjadi metode terbaik yang mampu meningkatkan kemampuan model secara menyeluruh. Z-Score tetap menjadi pilihan yang kuat, sementara Decimal Scaling sebaiknya dihindari untuk penggunaan KNN, terutama dalam kasus-kasus klasifikasi yang sensitif terhadap kesalahan identifikasi data negatif.




ROC AUC
Berdasarkan hasil evaluasi menggunakan metrik ROC AUC (Receiver Operating Characteristic - Area Under Curve), terlihat bahwa penggunaan metode normalisasi kembali memberikan pengaruh besar terhadap performa algoritma K-Nearest Neighbors (KNN). Metrik ROC AUC sangat penting karena menggambarkan kemampuan model dalam membedakan antara kelas positif dan negatif secara keseluruhan—semakin mendekati 1, semakin baik performa model.
Pada kondisi tanpa normalisasi, nilai ROC AUC tertinggi tercapai pada K = 11 dengan skor 0.7802, dan nilai terendah berada di K = 1 sebesar 0.7030. Meskipun terlihat adanya kenaikan ROC AUC seiring bertambahnya nilai K, nilainya masih tergolong sedang. Hal ini kembali menunjukkan bahwa ketika skala antar fitur tidak disamakan, perhitungan jarak dalam KNN menjadi kurang akurat, yang berdampak pada menurunnya kemampuan model dalam memisahkan dua kelas secara efisien.
Ketika menggunakan Min-Max Scaling, peningkatan performa sangat jelas terlihat. ROC AUC tertinggi tercapai pada K = 11 dengan skor 0.8050, sementara pada K = 9 nilainya hampir sama yaitu 0.8037. Mulai dari K = 3 hingga K = 9, skor ROC AUC selalu berada di atas 0.78, yang menunjukkan kestabilan dan kekuatan model dalam klasifikasi. Min-Max Scaling menyamakan semua fitur ke rentang nilai yang konsisten, sehingga memungkinkan KNN menghitung jarak antar titik dengan akurasi yang lebih baik—dampaknya, model menjadi lebih tajam dalam membedakan kelas.
Pada Z-Score (Standard Scaling), performa ROC AUC juga meningkat dengan baik, bahkan skor tertinggi mencapai 0.8129 pada K = 11—tertinggi dari seluruh kombinasi dalam eksperimen ini. Mulai dari K = 5 ke atas, skor selalu di atas 0.75 dan terus meningkat, menandakan bahwa Z-Score mampu menjaga performa model secara stabil sekaligus maksimal. Ini juga menunjukkan bahwa normalisasi berdasarkan distribusi (mean = 0, std = 1) efektif mengurangi dominasi fitur tertentu dan membuat model lebih adil dalam mengukur jarak antar titik data.
Sementara itu, Decimal Scaling masih menunjukkan peningkatan performa dibanding tanpa normalisasi, namun tetap menjadi metode dengan hasil terendah di antara ketiga metode normalisasi lainnya. Nilai ROC AUC tertinggi hanya 0.7874 pada K = 11, dan nilainya masih lebih rendah dibandingkan hasil tertinggi pada Min-Max dan Z-Score. Metode ini tampaknya tidak cukup presisi dalam menyamakan skala fitur, karena hanya bergantung pada penggeseran titik desimal tanpa mempertimbangkan distribusi atau range nilai yang sebenarnya.
Secara keseluruhan, evaluasi berdasarkan ROC AUC memperkuat temuan sebelumnya bahwa normalisasi merupakan bagian yang sangat krusial dalam meningkatkan performa KNN. Z-Score (Standard Scaling) terbukti memberikan hasil ROC AUC terbaik secara keseluruhan, diikuti sangat dekat oleh Min-Max Scaling. Keduanya mampu membuat KNN lebih sensitif dan selektif terhadap perbedaan antar kelas. Di sisi lain, Decimal Scaling tetap lebih baik dibanding tanpa normalisasi, tetapi kurang direkomendasikan karena tidak memberikan peningkatan yang signifikan. Untuk penerapan KNN dalam konteks klasifikasi yang memerlukan akurasi tinggi dalam membedakan kelas, pemilihan metode normalisasi yang tepat menjadi faktor penentu kesuksesan model.





Jadikan semua metrik evaluasi menjadi satu pembahasan, bandingkan juga pengaruh setiap metode normalisasi dengan yang tanpa normalisasi, lalu bandingkan juga mana metode normalisasi yang terbaik